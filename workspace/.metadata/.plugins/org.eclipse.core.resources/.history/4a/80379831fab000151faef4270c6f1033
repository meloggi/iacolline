package markov;

import java.util.ArrayList;

public class Algorithm {
	
	static double gamma = ValueIteration.gamma;   // gamma close to 1 to have a far sight to the reward 
	static double epsilon = ValueIteration.epsilon;
	static ArrayList<Double> reward = ValueIteration.reward;
	static ArrayList<ValueClass> Value = new ArrayList<ValueClass>(); // Values
	
	public int get_index_value(int speed, int position){
		return ((speed*ValueIteration.discretisation_position)+position);
	}
	
	public int get_speed_from_index(int index){
		
	}
	
	public void Value_calculate(int i){
		t = Value.get(i).ValueT.size();
		double new_value = Math.max(a, b) ;
		Value.get(i).ValueT.add(new_value);
	}
	
	
	
	public static void main(String args[]){
		
		
		
		/*
		
		
		for(int i = 0; i<ValueIteration.discretisation_position*ValueIteration.discretisation_speed;i++){
			// creation of values
			Value.add(new ValueClass(i)); // each state shall have its number
		}
		
		//loop on every state
		for(int i = 0; i<ValueIteration.discretisation_position*ValueIteration.discretisation_speed;i++){
			int t = 0;
			
			Value.get(i).ValueT.add(Math.max(0.0, 0.0)); // first value for moving forward, second for moving backward
			
			//loop of stage 3, page37, MDP course
			while(Math.abs(Value.get(i).ValueT.get(t+1)-Value.get(i).ValueT.get(t))>=epsilon*(1-gamma)/(2*gamma)){  
				t++;
				Value.get(i).ValueT.add(Math.max(0.0, 0.0));  // new value for t to satisfy the ValueIteration inequality
				//index should be correct (no unbound exception thrown)
			}
			
			Value.get(i).setQvalue_forward(reward + gamma*Value.get(i).ValueT.get(t));		// Qvalue moving forward
			Value.get(i).setQvalue_backward(reward + gamma*Value.get(i).ValueT.get(t)); // Qvalue moving backward
			
			// missing optimal policy
		}*/
		
	}

}
